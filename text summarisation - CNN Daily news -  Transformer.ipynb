{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e32ff5c9",
   "metadata": {},
   "source": [
    "## Transformer language model: Text summarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ff9d2",
   "metadata": {},
   "source": [
    "#### References:\n",
    "code sourced from: https://www.kaggle.com/code/ashishsingh226/text-summarization-with-transformers\n",
    "\n",
    "dataset from: https://paperswithcode.com/dataset/cnn-daily-mail-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b702865",
   "metadata": {},
   "source": [
    "### Import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8250fc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cljbu\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e9f4e",
   "metadata": {},
   "source": [
    "### Initalise hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f107ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_LEN = 100\n",
    "DECODER_LEN = 20\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = BATCH_SIZE*8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca620bf",
   "metadata": {},
   "source": [
    "### Import data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cdaa44ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\", '3.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2086c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing training set size to reduce training time\n",
    "article = dataset['train']['article'][0:130000]\n",
    "summary = dataset['train']['highlights'][0:130000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f75752",
   "metadata": {},
   "source": [
    "### Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98f5ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess the data\n",
    "def preprocess_text(text):\n",
    "\n",
    "    text = text.lower() # converting to lowercase\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # removing URL links\n",
    "    text = re.sub(r\"\\b\\d+\\b\", \"\", text) # removing number \n",
    "    text = re.sub('<.*?>+', '', text) # removing special characters, \n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # punctuations\n",
    "    text = re.sub('\\n', '', text) # newlines\n",
    "    text = re.sub('[’“”…]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)   \n",
    "\n",
    "    # removing short form: \n",
    "    text=re.sub(\"isn't\",'is not',text)\n",
    "    text=re.sub(\"he's\",'he is',text)\n",
    "    text=re.sub(\"wasn't\",'was not',text)\n",
    "    text=re.sub(\"there's\",'there is',text)\n",
    "    text=re.sub(\"couldn't\",'could not',text)\n",
    "    text=re.sub(\"won't\",'will not',text)\n",
    "    text=re.sub(\"they're\",'they are',text)\n",
    "    text=re.sub(\"she's\",'she is',text)\n",
    "    text=re.sub(\"There's\",'there is',text)\n",
    "    text=re.sub(\"wouldn't\",'would not',text)\n",
    "    text=re.sub(\"haven't\",'have not',text)\n",
    "    text=re.sub(\"That's\",'That is',text)\n",
    "    text=re.sub(\"you've\",'you have',text)\n",
    "    text=re.sub(\"He's\",'He is',text)\n",
    "    text=re.sub(\"what's\",'what is',text)\n",
    "    text=re.sub(\"weren't\",'were not',text)\n",
    "    text=re.sub(\"we're\",'we are',text)\n",
    "    text=re.sub(\"hasn't\",'has not',text)\n",
    "    text=re.sub(\"you'd\",'you would',text)\n",
    "    text=re.sub(\"shouldn't\",'should not',text)\n",
    "    text=re.sub(\"let's\",'let us',text)\n",
    "    text=re.sub(\"they've\",'they have',text)\n",
    "    text=re.sub(\"You'll\",'You will',text)\n",
    "    text=re.sub(\"i'm\",'i am',text)\n",
    "    text=re.sub(\"we've\",'we have',text)\n",
    "    text=re.sub(\"it's\",'it is',text)\n",
    "    text=re.sub(\"don't\",'do not',text)\n",
    "    text=re.sub(\"that´s\",'that is',text)\n",
    "    text=re.sub(\"I´m\",'I am',text)\n",
    "    text=re.sub(\"it’s\",'it is',text)\n",
    "    text=re.sub(\"she´s\",'she is',text)\n",
    "    text=re.sub(\"he’s'\",'he is',text)\n",
    "    text=re.sub('I’m','I am',text)\n",
    "    text=re.sub('I’d','I did',text)\n",
    "    text=re.sub(\"he’s'\",'he is',text)\n",
    "    text=re.sub('there’s','there is',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71139a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess every observation in training data\n",
    "article = [preprocess_text(x) for x in article]\n",
    "summary = [preprocess_text(x) for x in summary]\n",
    "\n",
    "# add start of sentence (sos) and end of sentence (eos) markers\n",
    "article = ['<SOS> ' + x + ' <EOS>' for x in article]\n",
    "summary = ['<SOS> ' + x + ' <EOS>' for x in summary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bb1f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise words\n",
    "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
    "oov_token = '<unk>'\n",
    "article_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
    "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)\n",
    "article_tokenizer.fit_on_texts(article)\n",
    "summary_tokenizer.fit_on_texts(summary)\n",
    "inputs = article_tokenizer.texts_to_sequences(article)\n",
    "targets = summary_tokenizer.texts_to_sequences(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5151dbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419836 129849\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary of articles and summaries\n",
    "ENCODER_VOCAB = len(article_tokenizer.word_index) + 1\n",
    "DECODER_VOCAB = len(summary_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1667a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inputs and targets\n",
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=ENCODER_LEN, padding='post', truncating='post')\n",
    "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=DECODER_LEN, padding='post', truncating='post')\n",
    "inputs = tf.cast(inputs, dtype=tf.int64)\n",
    "targets = tf.cast(targets, dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b292347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataset and create batches for training\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cdc068a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 1,\n",
       " 'the': 2,\n",
       " 'to': 3,\n",
       " '<sos>': 4,\n",
       " '<eos>': 5,\n",
       " 'in': 6,\n",
       " 'of': 7,\n",
       " 'a': 8,\n",
       " 'and': 9,\n",
       " 'for': 10,\n",
       " 'is': 11,\n",
       " 'says': 12,\n",
       " 'on': 13,\n",
       " 'was': 14,\n",
       " 'he': 15,\n",
       " 'with': 16,\n",
       " 'at': 17,\n",
       " 'his': 18,\n",
       " 'has': 19,\n",
       " 'new': 20,\n",
       " 'from': 21,\n",
       " 'are': 22,\n",
       " 'by': 23,\n",
       " 'as': 24,\n",
       " 'be': 25,\n",
       " 'have': 26,\n",
       " 'will': 27,\n",
       " 'her': 28,\n",
       " 'after': 29,\n",
       " 'it': 30,\n",
       " 'that': 31,\n",
       " 'she': 32,\n",
       " 'an': 33,\n",
       " 'been': 34,\n",
       " 'were': 35,\n",
       " 'not': 36,\n",
       " 'but': 37,\n",
       " 'say': 38,\n",
       " 'us': 39,\n",
       " 'had': 40,\n",
       " 'police': 41,\n",
       " 'more': 42,\n",
       " 'they': 43,\n",
       " 'one': 44,\n",
       " 'two': 45,\n",
       " 'said': 46,\n",
       " 'people': 47,\n",
       " 'its': 48,\n",
       " 'their': 49,\n",
       " 'about': 50,\n",
       " 'up': 51,\n",
       " 'years': 52,\n",
       " 'who': 53,\n",
       " 'out': 54,\n",
       " 'over': 55,\n",
       " 'this': 56,\n",
       " 'than': 57,\n",
       " 'first': 58,\n",
       " 'also': 59,\n",
       " 'no': 60,\n",
       " 'last': 61,\n",
       " 'into': 62,\n",
       " 'year': 63,\n",
       " 'found': 64,\n",
       " 'president': 65,\n",
       " 'when': 66,\n",
       " 'could': 67,\n",
       " 'world': 68,\n",
       " 'can': 69,\n",
       " 'him': 70,\n",
       " 'three': 71,\n",
       " 'or': 72,\n",
       " 'some': 73,\n",
       " 'obama': 74,\n",
       " 'home': 75,\n",
       " 'killed': 76,\n",
       " 'now': 77,\n",
       " 'would': 78,\n",
       " 'being': 79,\n",
       " 'death': 80,\n",
       " 'other': 81,\n",
       " 'time': 82,\n",
       " 'yearold': 83,\n",
       " 'before': 84,\n",
       " 'all': 85,\n",
       " 'family': 86,\n",
       " 'court': 87,\n",
       " 'since': 88,\n",
       " 'against': 89,\n",
       " 'former': 90,\n",
       " 'government': 91,\n",
       " 'may': 92,\n",
       " 'died': 93,\n",
       " 'million': 94,\n",
       " 'off': 95,\n",
       " 'man': 96,\n",
       " 'state': 97,\n",
       " 'show': 98,\n",
       " 'if': 99,\n",
       " 'there': 100,\n",
       " 'day': 101,\n",
       " 'during': 102,\n",
       " 'officials': 103,\n",
       " 'made': 104,\n",
       " 'most': 105,\n",
       " 'group': 106,\n",
       " 'i': 107,\n",
       " 'house': 108,\n",
       " 'four': 109,\n",
       " 'help': 110,\n",
       " 'official': 111,\n",
       " 'children': 112,\n",
       " 'we': 113,\n",
       " 'down': 114,\n",
       " 'many': 115,\n",
       " 'week': 116,\n",
       " 'only': 117,\n",
       " 'women': 118,\n",
       " 'just': 119,\n",
       " 'back': 120,\n",
       " 'city': 121,\n",
       " 'you': 122,\n",
       " 'which': 123,\n",
       " 'still': 124,\n",
       " 'them': 125,\n",
       " 'should': 126,\n",
       " 'south': 127,\n",
       " 'get': 128,\n",
       " 'attack': 129,\n",
       " 'life': 130,\n",
       " 'take': 131,\n",
       " 'report': 132,\n",
       " 'cnn': 133,\n",
       " 'while': 134,\n",
       " 'make': 135,\n",
       " 'like': 136,\n",
       " 'mother': 137,\n",
       " 'use': 138,\n",
       " 'second': 139,\n",
       " 'five': 140,\n",
       " 'authorities': 141,\n",
       " 'used': 142,\n",
       " 'arrested': 143,\n",
       " 'hospital': 144,\n",
       " 'between': 145,\n",
       " 'military': 146,\n",
       " 'accused': 147,\n",
       " 'told': 148,\n",
       " 'school': 149,\n",
       " 'claims': 150,\n",
       " 'under': 151,\n",
       " 'set': 152,\n",
       " 'days': 153,\n",
       " 'top': 154,\n",
       " 'work': 155,\n",
       " 'left': 156,\n",
       " 'calls': 157,\n",
       " 'months': 158,\n",
       " 'through': 159,\n",
       " 'case': 160,\n",
       " 'dead': 161,\n",
       " 'what': 162,\n",
       " 'states': 163,\n",
       " 'north': 164,\n",
       " 'security': 165,\n",
       " 'part': 166,\n",
       " 'woman': 167,\n",
       " 'how': 168,\n",
       " 'win': 169,\n",
       " 'national': 170,\n",
       " 'car': 171,\n",
       " 'men': 172,\n",
       " 'next': 173,\n",
       " 'because': 174,\n",
       " 'public': 175,\n",
       " 'health': 176,\n",
       " 'charges': 177,\n",
       " 'team': 178,\n",
       " 'minister': 179,\n",
       " 'united': 180,\n",
       " 'month': 181,\n",
       " 'so': 182,\n",
       " 'charged': 183,\n",
       " 'murder': 184,\n",
       " 'called': 185,\n",
       " 'wife': 186,\n",
       " 'both': 187,\n",
       " 'shot': 188,\n",
       " 'six': 189,\n",
       " 'around': 190,\n",
       " 'war': 191,\n",
       " 'law': 192,\n",
       " 'took': 193,\n",
       " 'another': 194,\n",
       " 'must': 195,\n",
       " 'least': 196,\n",
       " 'shows': 197,\n",
       " 'do': 198,\n",
       " 'father': 199,\n",
       " 'go': 200,\n",
       " 'american': 201,\n",
       " 'high': 202,\n",
       " 'media': 203,\n",
       " 'york': 204,\n",
       " 'news': 205,\n",
       " 'video': 206,\n",
       " 'british': 207,\n",
       " 'hit': 208,\n",
       " 'place': 209,\n",
       " 'where': 210,\n",
       " 'london': 211,\n",
       " 'china': 212,\n",
       " 'company': 213,\n",
       " 'judge': 214,\n",
       " 'john': 215,\n",
       " 'open': 216,\n",
       " 'your': 217,\n",
       " 'face': 218,\n",
       " 'son': 219,\n",
       " 'fire': 220,\n",
       " 'david': 221,\n",
       " 'body': 222,\n",
       " 'country': 223,\n",
       " 'near': 224,\n",
       " 'international': 225,\n",
       " 'among': 226,\n",
       " 'sunday': 227,\n",
       " 'way': 228,\n",
       " 'monday': 229,\n",
       " 'those': 230,\n",
       " 'money': 231,\n",
       " 'held': 232,\n",
       " 'rights': 233,\n",
       " 'later': 234,\n",
       " 'support': 235,\n",
       " 'students': 236,\n",
       " 'parents': 237,\n",
       " 'expected': 238,\n",
       " 'reports': 239,\n",
       " 'leader': 240,\n",
       " 'party': 241,\n",
       " 'including': 242,\n",
       " 'white': 243,\n",
       " 'want': 244,\n",
       " 'friday': 245,\n",
       " 'prison': 246,\n",
       " 'others': 247,\n",
       " 'child': 248,\n",
       " 'victims': 249,\n",
       " 'taken': 250,\n",
       " 'seen': 251,\n",
       " 'hours': 252,\n",
       " 'dont': 253,\n",
       " 'star': 254,\n",
       " 'uk': 255,\n",
       " 'times': 256,\n",
       " 'cup': 257,\n",
       " 'see': 258,\n",
       " 'end': 259,\n",
       " 'without': 260,\n",
       " 'night': 261,\n",
       " 'service': 262,\n",
       " 'went': 263,\n",
       " 'members': 264,\n",
       " 'comes': 265,\n",
       " 'released': 266,\n",
       " 'did': 267,\n",
       " 'best': 268,\n",
       " 'final': 269,\n",
       " 'bill': 270,\n",
       " 'weeks': 271,\n",
       " 'young': 272,\n",
       " 'chief': 273,\n",
       " 'incident': 274,\n",
       " 'third': 275,\n",
       " 'head': 276,\n",
       " 'lost': 277,\n",
       " 'daughter': 278,\n",
       " 'michael': 279,\n",
       " 'won': 280,\n",
       " 'tuesday': 281,\n",
       " 'any': 282,\n",
       " 'air': 283,\n",
       " 'violence': 284,\n",
       " 'attacks': 285,\n",
       " 'wants': 286,\n",
       " 'our': 287,\n",
       " 'move': 288,\n",
       " 'own': 289,\n",
       " 'saturday': 290,\n",
       " 'tells': 291,\n",
       " 'guilty': 292,\n",
       " 'political': 293,\n",
       " 'call': 294,\n",
       " 'officers': 295,\n",
       " 'change': 296,\n",
       " 'power': 297,\n",
       " 'trial': 298,\n",
       " 'ago': 299,\n",
       " 'film': 300,\n",
       " 'killing': 301,\n",
       " 'away': 302,\n",
       " 'number': 303,\n",
       " 'too': 304,\n",
       " 'need': 305,\n",
       " 'west': 306,\n",
       " 'campaign': 307,\n",
       " 'then': 308,\n",
       " 'leaders': 309,\n",
       " 'deal': 310,\n",
       " 'put': 311,\n",
       " 'water': 312,\n",
       " 'plans': 313,\n",
       " 'good': 314,\n",
       " 'each': 315,\n",
       " 'missing': 316,\n",
       " 'office': 317,\n",
       " 'reported': 318,\n",
       " 'known': 319,\n",
       " 'care': 320,\n",
       " 'student': 321,\n",
       " 'pay': 322,\n",
       " 'un': 323,\n",
       " 'never': 324,\n",
       " 'drug': 325,\n",
       " 'due': 326,\n",
       " 'until': 327,\n",
       " 'race': 328,\n",
       " 'find': 329,\n",
       " 'sex': 330,\n",
       " 'injured': 331,\n",
       " 'game': 332,\n",
       " 'suspect': 333,\n",
       " 'today': 334,\n",
       " 'facebook': 335,\n",
       " 'thursday': 336,\n",
       " 'wednesday': 337,\n",
       " 'investigation': 338,\n",
       " 'across': 339,\n",
       " 'come': 340,\n",
       " 'real': 341,\n",
       " 'tv': 342,\n",
       " 'early': 343,\n",
       " 'americans': 344,\n",
       " 'per': 345,\n",
       " 'long': 346,\n",
       " 'food': 347,\n",
       " 'such': 348,\n",
       " 'trying': 349,\n",
       " 'several': 350,\n",
       " 'system': 351,\n",
       " 'big': 352,\n",
       " 'social': 353,\n",
       " 'spokesman': 354,\n",
       " 'behind': 355,\n",
       " 'plan': 356,\n",
       " 'using': 357,\n",
       " 'become': 358,\n",
       " 'record': 359,\n",
       " 'league': 360,\n",
       " 'nearly': 361,\n",
       " 'play': 362,\n",
       " 'area': 363,\n",
       " 'even': 364,\n",
       " 'local': 365,\n",
       " 'federal': 366,\n",
       " 'site': 367,\n",
       " 'online': 368,\n",
       " 'husband': 369,\n",
       " 'same': 370,\n",
       " 'came': 371,\n",
       " 'much': 372,\n",
       " 'prime': 373,\n",
       " 'run': 374,\n",
       " 'girl': 375,\n",
       " 'alleged': 376,\n",
       " 'human': 377,\n",
       " 'crash': 378,\n",
       " 'given': 379,\n",
       " 'season': 380,\n",
       " 'my': 381,\n",
       " 'mr': 382,\n",
       " 'very': 383,\n",
       " 'flight': 384,\n",
       " 'groups': 385,\n",
       " 'include': 386,\n",
       " 'hes': 387,\n",
       " 'decision': 388,\n",
       " 'faces': 389,\n",
       " 'attorney': 390,\n",
       " 'officer': 391,\n",
       " 'iraq': 392,\n",
       " 'shooting': 393,\n",
       " 'going': 394,\n",
       " 'french': 395,\n",
       " 'plane': 396,\n",
       " 'eight': 397,\n",
       " 'working': 398,\n",
       " 'forces': 399,\n",
       " 'major': 400,\n",
       " 'workers': 401,\n",
       " 'lead': 402,\n",
       " 'chinese': 403,\n",
       " 'defense': 404,\n",
       " 'return': 405,\n",
       " 'friends': 406,\n",
       " 'troops': 407,\n",
       " 'seven': 408,\n",
       " 'black': 409,\n",
       " 'lawyer': 410,\n",
       " 'despite': 411,\n",
       " 'vote': 412,\n",
       " 'victim': 413,\n",
       " 'didnt': 414,\n",
       " 'live': 415,\n",
       " 'allegedly': 416,\n",
       " 'gop': 417,\n",
       " 'california': 418,\n",
       " 'experts': 419,\n",
       " 'right': 420,\n",
       " 'past': 421,\n",
       " 'likely': 422,\n",
       " 'convicted': 423,\n",
       " 'got': 424,\n",
       " 'iran': 425,\n",
       " 'recent': 426,\n",
       " 'couple': 427,\n",
       " 'department': 428,\n",
       " 'cancer': 429,\n",
       " 'university': 430,\n",
       " 'wins': 431,\n",
       " 'whether': 432,\n",
       " 'africa': 433,\n",
       " 'florida': 434,\n",
       " 'syria': 435,\n",
       " 'history': 436,\n",
       " 'thousands': 437,\n",
       " 'games': 438,\n",
       " 'park': 439,\n",
       " 'sent': 440,\n",
       " 'claim': 441,\n",
       " 'korea': 442,\n",
       " 'foreign': 443,\n",
       " 'every': 444,\n",
       " 'beat': 445,\n",
       " 'free': 446,\n",
       " 'already': 447,\n",
       " 'know': 448,\n",
       " 'march': 449,\n",
       " 'outside': 450,\n",
       " 'taking': 451,\n",
       " 'fans': 452,\n",
       " 'evidence': 453,\n",
       " 'girls': 454,\n",
       " 'key': 455,\n",
       " 'old': 456,\n",
       " 'fight': 457,\n",
       " 'al': 458,\n",
       " 'well': 459,\n",
       " 'issues': 460,\n",
       " 'earlier': 461,\n",
       " 'boy': 462,\n",
       " 'stop': 463,\n",
       " 'twitter': 464,\n",
       " 'event': 465,\n",
       " 'gun': 466,\n",
       " 'tried': 467,\n",
       " 'club': 468,\n",
       " 'give': 469,\n",
       " 'suffered': 470,\n",
       " 'secretary': 471,\n",
       " 'visit': 472,\n",
       " 'phone': 473,\n",
       " 'mark': 474,\n",
       " 'believe': 475,\n",
       " 'business': 476,\n",
       " 'france': 477,\n",
       " 'spent': 478,\n",
       " 'control': 479,\n",
       " 'wanted': 480,\n",
       " 'countries': 481,\n",
       " 'nations': 482,\n",
       " 'study': 483,\n",
       " 'program': 484,\n",
       " 'action': 485,\n",
       " 'tour': 486,\n",
       " 'staff': 487,\n",
       " 'remains': 488,\n",
       " 'clinton': 489,\n",
       " 'following': 490,\n",
       " 'doctors': 491,\n",
       " 'keep': 492,\n",
       " 'role': 493,\n",
       " 'leave': 494,\n",
       " 'search': 495,\n",
       " 'billion': 496,\n",
       " 'series': 497,\n",
       " 'percent': 498,\n",
       " 'force': 499,\n",
       " 'started': 500,\n",
       " 'election': 501,\n",
       " 'medical': 502,\n",
       " 'legal': 503,\n",
       " 'afghanistan': 504,\n",
       " 'russia': 505,\n",
       " 'texas': 506,\n",
       " 'issue': 507,\n",
       " 'agency': 508,\n",
       " 'half': 509,\n",
       " 'baby': 510,\n",
       " 'test': 511,\n",
       " 'james': 512,\n",
       " 'job': 513,\n",
       " 'arrest': 514,\n",
       " 'hotel': 515,\n",
       " 'age': 516,\n",
       " 'justice': 517,\n",
       " 'policy': 518,\n",
       " 'june': 519,\n",
       " 'airport': 520,\n",
       " 'players': 521,\n",
       " 'book': 522,\n",
       " 'thought': 523,\n",
       " 'according': 524,\n",
       " 'paul': 525,\n",
       " 'release': 526,\n",
       " 'himself': 527,\n",
       " 'title': 528,\n",
       " 'worlds': 529,\n",
       " 'european': 530,\n",
       " 'users': 531,\n",
       " 'list': 532,\n",
       " 'less': 533,\n",
       " 'asked': 534,\n",
       " 'oil': 535,\n",
       " 'talks': 536,\n",
       " 'abuse': 537,\n",
       " 'protesters': 538,\n",
       " 'january': 539,\n",
       " 'washington': 540,\n",
       " 'east': 541,\n",
       " 'start': 542,\n",
       " 'name': 543,\n",
       " 'july': 544,\n",
       " 'having': 545,\n",
       " 'information': 546,\n",
       " 'safety': 547,\n",
       " 'injuries': 548,\n",
       " 'street': 549,\n",
       " 'general': 550,\n",
       " 'problems': 551,\n",
       " 'center': 552,\n",
       " 'includes': 553,\n",
       " 'close': 554,\n",
       " 'late': 555,\n",
       " 'takes': 556,\n",
       " 'deaths': 557,\n",
       " 'senate': 558,\n",
       " 'led': 559,\n",
       " 'better': 560,\n",
       " 'ban': 561,\n",
       " 'russian': 562,\n",
       " 'little': 563,\n",
       " 'match': 564,\n",
       " 'residents': 565,\n",
       " 'cut': 566,\n",
       " 'congress': 567,\n",
       " 'act': 568,\n",
       " 'making': 569,\n",
       " 'claimed': 570,\n",
       " 'cases': 571,\n",
       " 'great': 572,\n",
       " 'became': 573,\n",
       " 'cause': 574,\n",
       " 'saying': 575,\n",
       " 'miles': 576,\n",
       " 'england': 577,\n",
       " 'doesnt': 578,\n",
       " 'jail': 579,\n",
       " 'page': 580,\n",
       " 'army': 581,\n",
       " 'manchester': 582,\n",
       " 'heart': 583,\n",
       " 'council': 584,\n",
       " 'look': 585,\n",
       " 'victory': 586,\n",
       " 'marriage': 587,\n",
       " 'believed': 588,\n",
       " 'named': 589,\n",
       " 'driver': 590,\n",
       " 'football': 591,\n",
       " 'gave': 592,\n",
       " 'began': 593,\n",
       " 'april': 594,\n",
       " 'island': 595,\n",
       " 'america': 596,\n",
       " 'israel': 597,\n",
       " 'makes': 598,\n",
       " 'director': 599,\n",
       " 'town': 600,\n",
       " 'red': 601,\n",
       " 'mexico': 602,\n",
       " 'helped': 603,\n",
       " 'involved': 604,\n",
       " 'looking': 605,\n",
       " 'caused': 606,\n",
       " 'cent': 607,\n",
       " 'expert': 608,\n",
       " 'received': 609,\n",
       " 'friend': 610,\n",
       " 'fighting': 611,\n",
       " 'opposition': 612,\n",
       " 'coast': 613,\n",
       " 'story': 614,\n",
       " 'forced': 615,\n",
       " 'jailed': 616,\n",
       " 'pm': 617,\n",
       " 'saw': 618,\n",
       " 'treatment': 619,\n",
       " 'nuclear': 620,\n",
       " 'soldiers': 621,\n",
       " 'space': 622,\n",
       " 'failed': 623,\n",
       " 'sexual': 624,\n",
       " 'champion': 625,\n",
       " 'economic': 626,\n",
       " 'charge': 627,\n",
       " 'george': 628,\n",
       " 'grand': 629,\n",
       " 'drugs': 630,\n",
       " 'popular': 631,\n",
       " 'aid': 632,\n",
       " 'music': 633,\n",
       " 'wont': 634,\n",
       " 'photos': 635,\n",
       " 'far': 636,\n",
       " 'latest': 637,\n",
       " 'cant': 638,\n",
       " 'mayor': 639,\n",
       " 'kids': 640,\n",
       " 'think': 641,\n",
       " 'community': 642,\n",
       " 'cost': 643,\n",
       " 'homes': 644,\n",
       " 'suicide': 645,\n",
       " 'september': 646,\n",
       " 'met': 647,\n",
       " 'border': 648,\n",
       " 'points': 649,\n",
       " 'obamas': 650,\n",
       " 'crime': 651,\n",
       " 'small': 652,\n",
       " 'does': 653,\n",
       " 'november': 654,\n",
       " 'passengers': 655,\n",
       " 'building': 656,\n",
       " 'pakistan': 657,\n",
       " 'prosecutors': 658,\n",
       " 'share': 659,\n",
       " 'denies': 660,\n",
       " 'daily': 661,\n",
       " 'region': 662,\n",
       " 'needs': 663,\n",
       " 'almost': 664,\n",
       " 'source': 665,\n",
       " 'risk': 666,\n",
       " 'martin': 667,\n",
       " 'condition': 668,\n",
       " 'bank': 669,\n",
       " 'questions': 670,\n",
       " 'jobs': 671,\n",
       " 'again': 672,\n",
       " 'king': 673,\n",
       " 'possible': 674,\n",
       " 'line': 675,\n",
       " 'travel': 676,\n",
       " 'sentenced': 677,\n",
       " 'rules': 678,\n",
       " 'families': 679,\n",
       " 'protests': 680,\n",
       " 'tax': 681,\n",
       " 'lives': 682,\n",
       " 'isis': 683,\n",
       " 'future': 684,\n",
       " 'african': 685,\n",
       " 'britain': 686,\n",
       " 'stay': 687,\n",
       " 'person': 688,\n",
       " 'fbi': 689,\n",
       " 'ever': 690,\n",
       " 'love': 691,\n",
       " 'morning': 692,\n",
       " 'private': 693,\n",
       " 'economy': 694,\n",
       " 'once': 695,\n",
       " 'williams': 696,\n",
       " 'announced': 697,\n",
       " 'admitted': 698,\n",
       " 'career': 699,\n",
       " 'jackson': 700,\n",
       " 'presidential': 701,\n",
       " 'sentence': 702,\n",
       " 'internet': 703,\n",
       " 'few': 704,\n",
       " 'assault': 705,\n",
       " 'ahead': 706,\n",
       " 'paid': 707,\n",
       " 'india': 708,\n",
       " 'taliban': 709,\n",
       " 'civil': 710,\n",
       " 'crisis': 711,\n",
       " 'storm': 712,\n",
       " 'brother': 713,\n",
       " 'brown': 714,\n",
       " 'player': 715,\n",
       " 'weapons': 716,\n",
       " 'allegations': 717,\n",
       " 'living': 718,\n",
       " 'often': 719,\n",
       " 'played': 720,\n",
       " 'republicans': 721,\n",
       " 'these': 722,\n",
       " 'romney': 723,\n",
       " 'remain': 724,\n",
       " 'member': 725,\n",
       " 'hopes': 726,\n",
       " 'nine': 727,\n",
       " 'december': 728,\n",
       " 'church': 729,\n",
       " 'meet': 730,\n",
       " 'hundreds': 731,\n",
       " 'recently': 732,\n",
       " 'minutes': 733,\n",
       " 'female': 734,\n",
       " 'hearing': 735,\n",
       " 'discovered': 736,\n",
       " 'weekend': 737,\n",
       " 'continue': 738,\n",
       " 'side': 739,\n",
       " 'operation': 740,\n",
       " 'europe': 741,\n",
       " 'movie': 742,\n",
       " 'market': 743,\n",
       " 'october': 744,\n",
       " 'special': 745,\n",
       " 'suspects': 746,\n",
       " 'companies': 747,\n",
       " 'apple': 748,\n",
       " 'turned': 749,\n",
       " 'sold': 750,\n",
       " 'appeal': 751,\n",
       " 'trip': 752,\n",
       " 'county': 753,\n",
       " 'patients': 754,\n",
       " 'meeting': 755,\n",
       " 'global': 756,\n",
       " 'full': 757,\n",
       " 'bush': 758,\n",
       " 'worked': 759,\n",
       " 'dog': 760,\n",
       " 'japan': 761,\n",
       " 'project': 762,\n",
       " 'offers': 763,\n",
       " 'hope': 764,\n",
       " 'sen': 765,\n",
       " 'coach': 766,\n",
       " 'born': 767,\n",
       " 'yet': 768,\n",
       " 'google': 769,\n",
       " 'royal': 770,\n",
       " 'owner': 771,\n",
       " 'august': 772,\n",
       " 'order': 773,\n",
       " 'dr': 774,\n",
       " 'getting': 775,\n",
       " 'kill': 776,\n",
       " 'scene': 777,\n",
       " 'rape': 778,\n",
       " 'democrats': 779,\n",
       " 'round': 780,\n",
       " 'attacked': 781,\n",
       " 'board': 782,\n",
       " 'fell': 783,\n",
       " 'capital': 784,\n",
       " 'technology': 785,\n",
       " 'married': 786,\n",
       " 'english': 787,\n",
       " 'robert': 788,\n",
       " 'stars': 789,\n",
       " 'reach': 790,\n",
       " 'station': 791,\n",
       " 'images': 792,\n",
       " 'february': 793,\n",
       " 'gold': 794,\n",
       " 'ruling': 795,\n",
       " 'union': 796,\n",
       " 'champions': 797,\n",
       " 'why': 798,\n",
       " 'offer': 799,\n",
       " 'winner': 800,\n",
       " 'accident': 801,\n",
       " 'de': 802,\n",
       " 'happened': 803,\n",
       " 'heard': 804,\n",
       " 'weather': 805,\n",
       " 'clear': 806,\n",
       " 'research': 807,\n",
       " 'comments': 808,\n",
       " 'emergency': 809,\n",
       " 'doctor': 810,\n",
       " 'german': 811,\n",
       " 'speech': 812,\n",
       " 'caught': 813,\n",
       " 'industry': 814,\n",
       " 'crew': 815,\n",
       " 'ship': 816,\n",
       " 'training': 817,\n",
       " 'disease': 818,\n",
       " 'enough': 819,\n",
       " 'problem': 820,\n",
       " 'running': 821,\n",
       " 'jury': 822,\n",
       " 'san': 823,\n",
       " 'watch': 824,\n",
       " 'biggest': 825,\n",
       " 'driving': 826,\n",
       " 'battle': 827,\n",
       " 'surgery': 828,\n",
       " 'together': 829,\n",
       " 'bomb': 830,\n",
       " 'gay': 831,\n",
       " 'largest': 832,\n",
       " 'launched': 833,\n",
       " 'pair': 834,\n",
       " 'writes': 835,\n",
       " 'prince': 836,\n",
       " 'data': 837,\n",
       " 'different': 838,\n",
       " 'within': 839,\n",
       " 'olympic': 840,\n",
       " 'boys': 841,\n",
       " 'suspected': 842,\n",
       " 'syrian': 843,\n",
       " 'serious': 844,\n",
       " 'criminal': 845,\n",
       " 'road': 846,\n",
       " 'debate': 847,\n",
       " 'inside': 848,\n",
       " 'able': 849,\n",
       " 'fired': 850,\n",
       " 'point': 851,\n",
       " 'education': 852,\n",
       " 'ground': 853,\n",
       " 'governor': 854,\n",
       " 'hold': 855,\n",
       " 'train': 856,\n",
       " 'transcript': 857,\n",
       " 'gas': 858,\n",
       " 'germany': 859,\n",
       " 'william': 860,\n",
       " 'large': 861,\n",
       " 'fourth': 862,\n",
       " 'australian': 863,\n",
       " 'bodies': 864,\n",
       " 'bring': 865,\n",
       " 'bad': 866,\n",
       " 'actor': 867,\n",
       " 'college': 868,\n",
       " 'peace': 869,\n",
       " 'services': 870,\n",
       " 'chance': 871,\n",
       " 'reportedly': 872,\n",
       " 'try': 873,\n",
       " 'central': 874,\n",
       " 'immigration': 875,\n",
       " 'damage': 876,\n",
       " 'threat': 877,\n",
       " 'korean': 878,\n",
       " 'details': 879,\n",
       " 'revealed': 880,\n",
       " 'needed': 881,\n",
       " 'rebels': 882,\n",
       " 'schools': 883,\n",
       " 'appeared': 884,\n",
       " 'access': 885,\n",
       " 'fear': 886,\n",
       " 'strike': 887,\n",
       " 'created': 888,\n",
       " 'me': 889,\n",
       " 'critics': 890,\n",
       " 'room': 891,\n",
       " 'madrid': 892,\n",
       " 'theres': 893,\n",
       " 'qaeda': 894,\n",
       " 'womens': 895,\n",
       " 'republican': 896,\n",
       " 'israeli': 897,\n",
       " 'middle': 898,\n",
       " 'investigators': 899,\n",
       " 'areas': 900,\n",
       " 'goal': 901,\n",
       " 'based': 902,\n",
       " 'returned': 903,\n",
       " 'strong': 904,\n",
       " 'northern': 905,\n",
       " 'investigating': 906,\n",
       " 'question': 907,\n",
       " 'fall': 908,\n",
       " 'supreme': 909,\n",
       " 'brain': 910,\n",
       " 'scores': 911,\n",
       " 'might': 912,\n",
       " 'believes': 913,\n",
       " 'worth': 914,\n",
       " 'italian': 915,\n",
       " 'militants': 916,\n",
       " 'host': 917,\n",
       " 'finds': 918,\n",
       " 'richard': 919,\n",
       " 'playing': 920,\n",
       " 'voters': 921,\n",
       " 'message': 922,\n",
       " 'along': 923,\n",
       " 'energy': 924,\n",
       " 'chris': 925,\n",
       " 'summer': 926,\n",
       " 'singer': 927,\n",
       " 'relationship': 928,\n",
       " 'blood': 929,\n",
       " 'front': 930,\n",
       " 'nation': 931,\n",
       " 'protest': 932,\n",
       " 'wounded': 933,\n",
       " 'talk': 934,\n",
       " 'tests': 935,\n",
       " 'parts': 936,\n",
       " 'post': 937,\n",
       " 'birth': 938,\n",
       " 'hard': 939,\n",
       " 'miss': 940,\n",
       " 'events': 941,\n",
       " 'personal': 942,\n",
       " 'leading': 943,\n",
       " 'illegal': 944,\n",
       " 'committee': 945,\n",
       " 'st': 946,\n",
       " 'moved': 947,\n",
       " 'launch': 948,\n",
       " 'let': 949,\n",
       " 'ordered': 950,\n",
       " 'australia': 951,\n",
       " 'huge': 952,\n",
       " 'tell': 953,\n",
       " 'chelsea': 954,\n",
       " 'teams': 955,\n",
       " 'brought': 956,\n",
       " 'growing': 957,\n",
       " 'art': 958,\n",
       " 'injury': 959,\n",
       " 'administration': 960,\n",
       " 'barack': 961,\n",
       " 'rise': 962,\n",
       " 'similar': 963,\n",
       " 'cars': 964,\n",
       " 'response': 965,\n",
       " 'allow': 966,\n",
       " 'laws': 967,\n",
       " 'christmas': 968,\n",
       " 'bus': 969,\n",
       " 'focus': 970,\n",
       " 'suspended': 971,\n",
       " 'current': 972,\n",
       " 'financial': 973,\n",
       " 'stories': 974,\n",
       " 'draw': 975,\n",
       " 'increase': 976,\n",
       " 'sign': 977,\n",
       " 'allowed': 978,\n",
       " 'winning': 979,\n",
       " 'save': 980,\n",
       " 'twice': 981,\n",
       " 'airlines': 982,\n",
       " 'firm': 983,\n",
       " 'denied': 984,\n",
       " 'cameron': 985,\n",
       " 'create': 986,\n",
       " 'charity': 987,\n",
       " 'southern': 988,\n",
       " 'success': 989,\n",
       " 'camp': 990,\n",
       " 'costs': 991,\n",
       " 'van': 992,\n",
       " 'turn': 993,\n",
       " 'gets': 994,\n",
       " 'jersey': 995,\n",
       " 'muslim': 996,\n",
       " 'jones': 997,\n",
       " 'planned': 998,\n",
       " 'scientists': 999,\n",
       " 'poor': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of all words with frequencies\n",
    "summary_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2ed67",
   "metadata": {},
   "source": [
    "### Vanilla Transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bdd03f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(position, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return position * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "671db7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating multi-head attention layers\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "            \n",
    "        return output, attention_weights\n",
    "\n",
    "# feedforward layer\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d8d9b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder layer\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaeda41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        \n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4315acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "        return x\n",
    "\n",
    "# decoder\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "        \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        \n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98163473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer, made from encoder, decoder and final layer\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f55c9819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise hyperparameters\n",
    "num_layers = 3\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 4\n",
    "dropout_rate = 0.2\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2340d4",
   "metadata": {},
   "source": [
    "### Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff6fe30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the Adam optimisation algorithm\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5509c131",
   "metadata": {},
   "source": [
    "### Loss and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6824cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "    #accuracies = tf.cast(accuracies, dtype= tf.float32)\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2817792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6eba9600",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=ENCODER_VOCAB,\n",
    "    target_vocab_size=DECODER_VOCAB,\n",
    "    pe_input=1000,\n",
    "    pe_target=1000,\n",
    "    rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8aa9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "668be9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "041b9169",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(\n",
    "            inp, tar_inp, \n",
    "            True, \n",
    "            enc_padding_mask, \n",
    "            combined_mask, \n",
    "            dec_padding_mask\n",
    "        )\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1564c950",
   "metadata": {},
   "source": [
    "### Training the model:\n",
    "Takes ~45 hours to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be020b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 11.7806 Accuracy 0.0000\n",
      "Epoch 1 Batch 100 Loss 8.8965 Accuracy 0.0252\n",
      "Epoch 1 Batch 200 Loss 8.5267 Accuracy 0.0290\n",
      "Epoch 1 Batch 300 Loss 8.2676 Accuracy 0.0380\n",
      "Epoch 1 Batch 400 Loss 8.1097 Accuracy 0.0436\n",
      "Epoch 1 Batch 500 Loss 8.0122 Accuracy 0.0471\n",
      "Epoch 1 Batch 600 Loss 7.9364 Accuracy 0.0500\n",
      "Epoch 1 Batch 700 Loss 7.8654 Accuracy 0.0533\n",
      "Epoch 1 Batch 800 Loss 7.8081 Accuracy 0.0563\n",
      "Epoch 1 Batch 900 Loss 7.7607 Accuracy 0.0594\n",
      "Epoch 1 Batch 1000 Loss 7.7197 Accuracy 0.0622\n",
      "Epoch 1 Batch 1100 Loss 7.6789 Accuracy 0.0651\n",
      "Epoch 1 Batch 1200 Loss 7.6351 Accuracy 0.0681\n",
      "Epoch 1 Batch 1300 Loss 7.5986 Accuracy 0.0703\n",
      "Epoch 1 Batch 1400 Loss 7.5580 Accuracy 0.0727\n",
      "Epoch 1 Batch 1500 Loss 7.5525 Accuracy 0.0735\n",
      "Epoch 1 Batch 1600 Loss 7.5341 Accuracy 0.0749\n",
      "Epoch 1 Batch 1700 Loss 7.5136 Accuracy 0.0764\n",
      "Epoch 1 Batch 1800 Loss 7.4923 Accuracy 0.0779\n",
      "Epoch 1 Batch 1900 Loss 7.4712 Accuracy 0.0793\n",
      "Epoch 1 Batch 2000 Loss 7.4494 Accuracy 0.0808\n",
      "Epoch 1 Loss 7.4424 Accuracy 0.0813\n",
      "Time taken for 1 epoch: 6012.1538252830505 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 7.4833 Accuracy 0.0813\n",
      "Epoch 2 Batch 100 Loss 7.1758 Accuracy 0.0820\n",
      "Epoch 2 Batch 200 Loss 7.0840 Accuracy 0.0830\n",
      "Epoch 2 Batch 300 Loss 6.9296 Accuracy 0.0847\n",
      "Epoch 2 Batch 400 Loss 6.8505 Accuracy 0.0863\n",
      "Epoch 2 Batch 500 Loss 6.8133 Accuracy 0.0878\n",
      "Epoch 2 Batch 600 Loss 6.7806 Accuracy 0.0892\n",
      "Epoch 2 Batch 700 Loss 6.7471 Accuracy 0.0907\n",
      "Epoch 2 Batch 800 Loss 6.7267 Accuracy 0.0921\n",
      "Epoch 2 Batch 900 Loss 6.7146 Accuracy 0.0933\n",
      "Epoch 2 Batch 1000 Loss 6.7067 Accuracy 0.0944\n",
      "Epoch 2 Batch 1100 Loss 6.6961 Accuracy 0.0955\n",
      "Epoch 2 Batch 1200 Loss 6.6787 Accuracy 0.0970\n",
      "Epoch 2 Batch 1300 Loss 6.6684 Accuracy 0.0979\n",
      "Epoch 2 Batch 1400 Loss 6.6499 Accuracy 0.0991\n",
      "Epoch 2 Batch 1500 Loss 6.6611 Accuracy 0.0993\n",
      "Epoch 2 Batch 1600 Loss 6.6609 Accuracy 0.0999\n",
      "Epoch 2 Batch 1700 Loss 6.6594 Accuracy 0.1005\n",
      "Epoch 2 Batch 1800 Loss 6.6577 Accuracy 0.1011\n",
      "Epoch 2 Batch 1900 Loss 6.6561 Accuracy 0.1017\n",
      "Epoch 2 Batch 2000 Loss 6.6529 Accuracy 0.1024\n",
      "Epoch 2 Loss 6.6515 Accuracy 0.1026\n",
      "Time taken for 1 epoch: 6167.61199593544 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 7.0722 Accuracy 0.1026\n",
      "Epoch 3 Batch 100 Loss 6.6933 Accuracy 0.1030\n",
      "Epoch 3 Batch 200 Loss 6.6208 Accuracy 0.1036\n",
      "Epoch 3 Batch 300 Loss 6.4801 Accuracy 0.1047\n",
      "Epoch 3 Batch 400 Loss 6.4095 Accuracy 0.1057\n",
      "Epoch 3 Batch 500 Loss 6.3789 Accuracy 0.1066\n",
      "Epoch 3 Batch 600 Loss 6.3536 Accuracy 0.1075\n",
      "Epoch 3 Batch 700 Loss 6.3260 Accuracy 0.1086\n",
      "Epoch 3 Batch 800 Loss 6.3127 Accuracy 0.1095\n",
      "Epoch 3 Batch 900 Loss 6.3066 Accuracy 0.1103\n",
      "Epoch 3 Batch 1000 Loss 6.3033 Accuracy 0.1110\n",
      "Epoch 3 Batch 1100 Loss 6.2985 Accuracy 0.1118\n",
      "Epoch 3 Batch 1200 Loss 6.2864 Accuracy 0.1127\n",
      "Epoch 3 Batch 1300 Loss 6.2806 Accuracy 0.1133\n",
      "Epoch 3 Batch 1400 Loss 6.2681 Accuracy 0.1141\n",
      "Epoch 3 Batch 1500 Loss 6.2854 Accuracy 0.1143\n",
      "Epoch 3 Batch 1600 Loss 6.2914 Accuracy 0.1146\n",
      "Epoch 3 Batch 1700 Loss 6.2958 Accuracy 0.1150\n",
      "Epoch 3 Batch 1800 Loss 6.2991 Accuracy 0.1154\n",
      "Epoch 3 Batch 1900 Loss 6.3017 Accuracy 0.1158\n",
      "Epoch 3 Batch 2000 Loss 6.3027 Accuracy 0.1163\n",
      "Epoch 3 Loss 6.3027 Accuracy 0.1164\n",
      "Time taken for 1 epoch: 6029.093722820282 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 6.6497 Accuracy 0.1164\n",
      "Epoch 4 Batch 100 Loss 6.4093 Accuracy 0.1167\n",
      "Epoch 4 Batch 200 Loss 6.3363 Accuracy 0.1172\n",
      "Epoch 4 Batch 300 Loss 6.1988 Accuracy 0.1180\n",
      "Epoch 4 Batch 400 Loss 6.1339 Accuracy 0.1187\n",
      "Epoch 4 Batch 500 Loss 6.1078 Accuracy 0.1194\n",
      "Epoch 4 Batch 600 Loss 6.0874 Accuracy 0.1201\n",
      "Epoch 4 Batch 700 Loss 6.0634 Accuracy 0.1209\n",
      "Epoch 4 Batch 800 Loss 6.0547 Accuracy 0.1216\n",
      "Epoch 4 Batch 900 Loss 6.0531 Accuracy 0.1221\n",
      "Epoch 4 Batch 1000 Loss 6.0535 Accuracy 0.1227\n",
      "Epoch 4 Batch 1100 Loss 6.0538 Accuracy 0.1232\n",
      "Epoch 4 Batch 1200 Loss 6.0451 Accuracy 0.1239\n",
      "Epoch 4 Batch 1300 Loss 6.0417 Accuracy 0.1244\n",
      "Epoch 4 Batch 1400 Loss 6.0314 Accuracy 0.1250\n",
      "Epoch 4 Batch 1500 Loss 6.0536 Accuracy 0.1251\n",
      "Epoch 4 Batch 1600 Loss 6.0636 Accuracy 0.1254\n",
      "Epoch 4 Batch 1700 Loss 6.0716 Accuracy 0.1257\n",
      "Epoch 4 Batch 1800 Loss 6.0772 Accuracy 0.1260\n",
      "Epoch 4 Batch 1900 Loss 6.0821 Accuracy 0.1263\n",
      "Epoch 4 Batch 2000 Loss 6.0850 Accuracy 0.1266\n",
      "Epoch 4 Loss 6.0856 Accuracy 0.1267\n",
      "Time taken for 1 epoch: 6028.138610839844 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 6.5021 Accuracy 0.1267\n",
      "Epoch 5 Batch 100 Loss 6.2106 Accuracy 0.1270\n",
      "Epoch 5 Batch 200 Loss 6.1440 Accuracy 0.1274\n",
      "Epoch 5 Batch 300 Loss 6.0163 Accuracy 0.1280\n",
      "Epoch 5 Batch 400 Loss 5.9539 Accuracy 0.1286\n",
      "Epoch 5 Batch 500 Loss 5.9335 Accuracy 0.1291\n",
      "Epoch 5 Batch 600 Loss 5.9157 Accuracy 0.1297\n",
      "Epoch 5 Batch 700 Loss 5.8946 Accuracy 0.1303\n",
      "Epoch 5 Batch 800 Loss 5.8870 Accuracy 0.1308\n",
      "Epoch 5 Batch 900 Loss 5.8867 Accuracy 0.1313\n",
      "Epoch 5 Batch 1000 Loss 5.8894 Accuracy 0.1317\n",
      "Epoch 5 Batch 1100 Loss 5.8910 Accuracy 0.1321\n",
      "Epoch 5 Batch 1200 Loss 5.8832 Accuracy 0.1327\n",
      "Epoch 5 Batch 1300 Loss 5.8821 Accuracy 0.1330\n",
      "Epoch 5 Batch 1400 Loss 5.8736 Accuracy 0.1335\n",
      "Epoch 5 Batch 1500 Loss 5.8979 Accuracy 0.1336\n",
      "Epoch 5 Batch 1600 Loss 5.9102 Accuracy 0.1338\n",
      "Epoch 5 Batch 1700 Loss 5.9195 Accuracy 0.1341\n",
      "Epoch 5 Batch 1800 Loss 5.9269 Accuracy 0.1343\n",
      "Epoch 5 Batch 1900 Loss 5.9328 Accuracy 0.1346\n",
      "Epoch 5 Batch 2000 Loss 5.9365 Accuracy 0.1348\n",
      "Saving checkpoint for epoch 5 at checkpoints\\ckpt-1\n",
      "Epoch 5 Loss 5.9373 Accuracy 0.1349\n",
      "Time taken for 1 epoch: 6036.576920509338 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 6.3103 Accuracy 0.1349\n",
      "Epoch 6 Batch 100 Loss 6.0609 Accuracy 0.1352\n",
      "Epoch 6 Batch 200 Loss 6.0079 Accuracy 0.1355\n",
      "Epoch 6 Batch 300 Loss 5.8869 Accuracy 0.1360\n",
      "Epoch 6 Batch 400 Loss 5.8294 Accuracy 0.1365\n",
      "Epoch 6 Batch 500 Loss 5.8113 Accuracy 0.1369\n",
      "Epoch 6 Batch 600 Loss 5.7956 Accuracy 0.1374\n",
      "Epoch 6 Batch 700 Loss 5.7755 Accuracy 0.1379\n",
      "Epoch 6 Batch 800 Loss 5.7687 Accuracy 0.1383\n",
      "Epoch 6 Batch 900 Loss 5.7687 Accuracy 0.1387\n",
      "Epoch 6 Batch 1000 Loss 5.7723 Accuracy 0.1391\n",
      "Epoch 6 Batch 1100 Loss 5.7743 Accuracy 0.1394\n",
      "Epoch 6 Batch 1200 Loss 5.7676 Accuracy 0.1399\n",
      "Epoch 6 Batch 1300 Loss 5.7668 Accuracy 0.1402\n",
      "Epoch 6 Batch 1400 Loss 5.7596 Accuracy 0.1406\n",
      "Epoch 6 Batch 1500 Loss 5.7857 Accuracy 0.1407\n",
      "Epoch 6 Batch 1600 Loss 5.7987 Accuracy 0.1409\n",
      "Epoch 6 Batch 1700 Loss 5.8098 Accuracy 0.1411\n",
      "Epoch 6 Batch 1800 Loss 5.8184 Accuracy 0.1413\n",
      "Epoch 6 Batch 1900 Loss 5.8245 Accuracy 0.1415\n",
      "Epoch 6 Batch 2000 Loss 5.8289 Accuracy 0.1418\n",
      "Epoch 6 Loss 5.8293 Accuracy 0.1418\n",
      "Time taken for 1 epoch: 6061.483461856842 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 6.2425 Accuracy 0.1418\n",
      "Epoch 7 Batch 100 Loss 5.9620 Accuracy 0.1421\n",
      "Epoch 7 Batch 200 Loss 5.9137 Accuracy 0.1423\n",
      "Epoch 7 Batch 300 Loss 5.7944 Accuracy 0.1428\n",
      "Epoch 7 Batch 400 Loss 5.7436 Accuracy 0.1432\n",
      "Epoch 7 Batch 500 Loss 5.7280 Accuracy 0.1436\n",
      "Epoch 7 Batch 600 Loss 5.7138 Accuracy 0.1440\n",
      "Epoch 7 Batch 700 Loss 5.6969 Accuracy 0.1444\n",
      "Epoch 7 Batch 800 Loss 5.6919 Accuracy 0.1448\n",
      "Epoch 7 Batch 900 Loss 5.6944 Accuracy 0.1451\n",
      "Epoch 7 Batch 1000 Loss 5.6993 Accuracy 0.1454\n",
      "Epoch 7 Batch 1100 Loss 5.7017 Accuracy 0.1457\n",
      "Epoch 7 Batch 1200 Loss 5.6961 Accuracy 0.1461\n",
      "Epoch 7 Batch 1300 Loss 5.6963 Accuracy 0.1464\n",
      "Epoch 7 Batch 1400 Loss 5.6883 Accuracy 0.1467\n",
      "Epoch 7 Batch 1500 Loss 5.7165 Accuracy 0.1468\n",
      "Epoch 7 Batch 1600 Loss 5.7315 Accuracy 0.1470\n",
      "Epoch 7 Batch 1700 Loss 5.7425 Accuracy 0.1472\n",
      "Epoch 7 Batch 1800 Loss 5.7495 Accuracy 0.1474\n",
      "Epoch 7 Batch 1900 Loss 5.7559 Accuracy 0.1476\n",
      "Epoch 7 Batch 2000 Loss 5.7601 Accuracy 0.1478\n",
      "Epoch 7 Loss 5.7610 Accuracy 0.1479\n",
      "Time taken for 1 epoch: 6007.360653400421 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 6.3374 Accuracy 0.1479\n",
      "Epoch 8 Batch 100 Loss 5.9022 Accuracy 0.1481\n",
      "Epoch 8 Batch 200 Loss 5.8565 Accuracy 0.1483\n",
      "Epoch 8 Batch 300 Loss 5.7467 Accuracy 0.1487\n",
      "Epoch 8 Batch 400 Loss 5.7040 Accuracy 0.1491\n",
      "Epoch 8 Batch 500 Loss 5.6934 Accuracy 0.1494\n",
      "Epoch 8 Batch 600 Loss 5.6832 Accuracy 0.1497\n",
      "Epoch 8 Batch 700 Loss 5.6687 Accuracy 0.1501\n",
      "Epoch 8 Batch 800 Loss 5.6647 Accuracy 0.1504\n",
      "Epoch 8 Batch 900 Loss 5.6667 Accuracy 0.1507\n",
      "Epoch 8 Batch 1000 Loss 5.6722 Accuracy 0.1510\n",
      "Epoch 8 Batch 1100 Loss 5.6773 Accuracy 0.1513\n",
      "Epoch 8 Batch 1200 Loss 5.6734 Accuracy 0.1516\n",
      "Epoch 8 Batch 1300 Loss 5.6742 Accuracy 0.1519\n",
      "Epoch 8 Batch 1400 Loss 5.6677 Accuracy 0.1522\n",
      "Epoch 8 Batch 1500 Loss 5.6952 Accuracy 0.1522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 1600 Loss 5.7072 Accuracy 0.1524\n",
      "Epoch 8 Batch 1700 Loss 5.7146 Accuracy 0.1526\n",
      "Epoch 8 Batch 1800 Loss 5.7223 Accuracy 0.1527\n",
      "Epoch 8 Batch 1900 Loss 5.7298 Accuracy 0.1529\n",
      "Epoch 8 Batch 2000 Loss 5.7365 Accuracy 0.1531\n",
      "Epoch 8 Loss 5.7384 Accuracy 0.1532\n",
      "Time taken for 1 epoch: 5381.612191200256 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 6.2637 Accuracy 0.1532\n",
      "Epoch 9 Batch 100 Loss 5.9018 Accuracy 0.1534\n",
      "Epoch 9 Batch 200 Loss 5.8573 Accuracy 0.1536\n",
      "Epoch 9 Batch 300 Loss 5.7400 Accuracy 0.1539\n",
      "Epoch 9 Batch 400 Loss 5.6827 Accuracy 0.1543\n",
      "Epoch 9 Batch 500 Loss 5.6693 Accuracy 0.1546\n",
      "Epoch 9 Batch 600 Loss 5.6581 Accuracy 0.1549\n",
      "Epoch 9 Batch 700 Loss 5.6475 Accuracy 0.1552\n",
      "Epoch 9 Batch 800 Loss 5.6488 Accuracy 0.1555\n",
      "Epoch 9 Batch 900 Loss 5.6571 Accuracy 0.1558\n",
      "Epoch 9 Batch 1000 Loss 5.6643 Accuracy 0.1560\n",
      "Epoch 9 Batch 1100 Loss 5.6697 Accuracy 0.1563\n",
      "Epoch 9 Batch 1200 Loss 5.6641 Accuracy 0.1566\n",
      "Epoch 9 Batch 1300 Loss 5.6618 Accuracy 0.1568\n",
      "Epoch 9 Batch 1400 Loss 5.6510 Accuracy 0.1571\n",
      "Epoch 9 Batch 1500 Loss 5.6753 Accuracy 0.1571\n",
      "Epoch 9 Batch 1600 Loss 5.6893 Accuracy 0.1573\n",
      "Epoch 9 Batch 1700 Loss 5.7040 Accuracy 0.1574\n",
      "Epoch 9 Batch 1800 Loss 5.7187 Accuracy 0.1576\n",
      "Epoch 9 Batch 1900 Loss 5.7316 Accuracy 0.1578\n",
      "Epoch 9 Batch 2000 Loss 5.7413 Accuracy 0.1580\n",
      "Epoch 9 Loss 5.7438 Accuracy 0.1580\n",
      "Time taken for 1 epoch: 5351.049249172211 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 6.2402 Accuracy 0.1580\n",
      "Epoch 10 Batch 100 Loss 5.8535 Accuracy 0.1582\n",
      "Epoch 10 Batch 200 Loss 5.7895 Accuracy 0.1584\n",
      "Epoch 10 Batch 300 Loss 5.6696 Accuracy 0.1587\n",
      "Epoch 10 Batch 400 Loss 5.6260 Accuracy 0.1590\n",
      "Epoch 10 Batch 500 Loss 5.6228 Accuracy 0.1593\n",
      "Epoch 10 Batch 600 Loss 5.6192 Accuracy 0.1596\n",
      "Epoch 10 Batch 700 Loss 5.6093 Accuracy 0.1599\n",
      "Epoch 10 Batch 800 Loss 5.6113 Accuracy 0.1601\n",
      "Epoch 10 Batch 900 Loss 5.6178 Accuracy 0.1604\n",
      "Epoch 10 Batch 1000 Loss 5.6236 Accuracy 0.1606\n",
      "Epoch 10 Batch 1100 Loss 5.6261 Accuracy 0.1608\n",
      "Epoch 10 Batch 1200 Loss 5.6177 Accuracy 0.1611\n",
      "Epoch 10 Batch 1300 Loss 5.6153 Accuracy 0.1613\n",
      "Epoch 10 Batch 1400 Loss 5.6067 Accuracy 0.1615\n",
      "Epoch 10 Batch 1500 Loss 5.6412 Accuracy 0.1616\n",
      "Epoch 10 Batch 1600 Loss 5.6677 Accuracy 0.1617\n",
      "Epoch 10 Batch 1700 Loss 5.6899 Accuracy 0.1619\n",
      "Epoch 10 Batch 1800 Loss 5.7062 Accuracy 0.1621\n",
      "Epoch 10 Batch 1900 Loss 5.7184 Accuracy 0.1622\n",
      "Epoch 10 Batch 2000 Loss 5.7258 Accuracy 0.1624\n",
      "Saving checkpoint for epoch 10 at checkpoints\\ckpt-2\n",
      "Epoch 10 Loss 5.7269 Accuracy 0.1625\n",
      "Time taken for 1 epoch: 5333.335351705551 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 6.0581 Accuracy 0.1625\n",
      "Epoch 11 Batch 100 Loss 5.7456 Accuracy 0.1626\n",
      "Epoch 11 Batch 200 Loss 5.7074 Accuracy 0.1628\n",
      "Epoch 11 Batch 300 Loss 5.6193 Accuracy 0.1631\n",
      "Epoch 11 Batch 400 Loss 5.5905 Accuracy 0.1634\n",
      "Epoch 11 Batch 500 Loss 5.5918 Accuracy 0.1636\n",
      "Epoch 11 Batch 600 Loss 5.5881 Accuracy 0.1639\n",
      "Epoch 11 Batch 700 Loss 5.5764 Accuracy 0.1642\n",
      "Epoch 11 Batch 800 Loss 5.5755 Accuracy 0.1644\n",
      "Epoch 11 Batch 900 Loss 5.5772 Accuracy 0.1646\n",
      "Epoch 11 Batch 1000 Loss 5.5788 Accuracy 0.1648\n",
      "Epoch 11 Batch 1100 Loss 5.5776 Accuracy 0.1650\n",
      "Epoch 11 Batch 1200 Loss 5.5705 Accuracy 0.1653\n",
      "Epoch 11 Batch 1300 Loss 5.5705 Accuracy 0.1655\n",
      "Epoch 11 Batch 1400 Loss 5.5642 Accuracy 0.1657\n",
      "Epoch 11 Batch 1500 Loss 5.6039 Accuracy 0.1658\n",
      "Epoch 11 Batch 1600 Loss 5.6314 Accuracy 0.1659\n",
      "Epoch 11 Batch 1700 Loss 5.6530 Accuracy 0.1661\n",
      "Epoch 11 Batch 1800 Loss 5.6675 Accuracy 0.1662\n",
      "Epoch 11 Batch 1900 Loss 5.6772 Accuracy 0.1664\n",
      "Epoch 11 Batch 2000 Loss 5.6815 Accuracy 0.1665\n",
      "Epoch 11 Loss 5.6820 Accuracy 0.1666\n",
      "Time taken for 1 epoch: 5508.728230714798 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 5.8717 Accuracy 0.1666\n",
      "Epoch 12 Batch 100 Loss 5.7111 Accuracy 0.1668\n",
      "Epoch 12 Batch 200 Loss 5.7077 Accuracy 0.1669\n",
      "Epoch 12 Batch 300 Loss 5.6250 Accuracy 0.1672\n",
      "Epoch 12 Batch 400 Loss 5.5912 Accuracy 0.1674\n",
      "Epoch 12 Batch 500 Loss 5.5825 Accuracy 0.1677\n",
      "Epoch 12 Batch 600 Loss 5.5734 Accuracy 0.1679\n",
      "Epoch 12 Batch 700 Loss 5.5544 Accuracy 0.1682\n",
      "Epoch 12 Batch 800 Loss 5.5453 Accuracy 0.1684\n",
      "Epoch 12 Batch 900 Loss 5.5398 Accuracy 0.1686\n",
      "Epoch 12 Batch 1000 Loss 5.5390 Accuracy 0.1688\n",
      "Epoch 12 Batch 1100 Loss 5.5396 Accuracy 0.1690\n",
      "Epoch 12 Batch 1200 Loss 5.5347 Accuracy 0.1692\n",
      "Epoch 12 Batch 1300 Loss 5.5370 Accuracy 0.1694\n",
      "Epoch 12 Batch 1400 Loss 5.5321 Accuracy 0.1696\n",
      "Epoch 12 Batch 1500 Loss 5.5717 Accuracy 0.1697\n",
      "Epoch 12 Batch 1600 Loss 5.5978 Accuracy 0.1698\n",
      "Epoch 12 Batch 1700 Loss 5.6160 Accuracy 0.1700\n",
      "Epoch 12 Batch 1800 Loss 5.6270 Accuracy 0.1701\n",
      "Epoch 12 Batch 1900 Loss 5.6337 Accuracy 0.1703\n",
      "Epoch 12 Batch 2000 Loss 5.6367 Accuracy 0.1704\n",
      "Epoch 12 Loss 5.6371 Accuracy 0.1705\n",
      "Time taken for 1 epoch: 6084.354982376099 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 6.0400 Accuracy 0.1705\n",
      "Epoch 13 Batch 100 Loss 5.7120 Accuracy 0.1706\n",
      "Epoch 13 Batch 200 Loss 5.7040 Accuracy 0.1708\n",
      "Epoch 13 Batch 300 Loss 5.6143 Accuracy 0.1710\n",
      "Epoch 13 Batch 400 Loss 5.5708 Accuracy 0.1713\n",
      "Epoch 13 Batch 500 Loss 5.5561 Accuracy 0.1715\n",
      "Epoch 13 Batch 600 Loss 5.5374 Accuracy 0.1717\n",
      "Epoch 13 Batch 700 Loss 5.5125 Accuracy 0.1720\n",
      "Epoch 13 Batch 800 Loss 5.4986 Accuracy 0.1722\n",
      "Epoch 13 Batch 900 Loss 5.4937 Accuracy 0.1724\n",
      "Epoch 13 Batch 1000 Loss 5.4955 Accuracy 0.1726\n",
      "Epoch 13 Batch 1100 Loss 5.5007 Accuracy 0.1728\n",
      "Epoch 13 Batch 1200 Loss 5.4987 Accuracy 0.1730\n",
      "Epoch 13 Batch 1300 Loss 5.5020 Accuracy 0.1731\n",
      "Epoch 13 Batch 1400 Loss 5.4981 Accuracy 0.1733\n",
      "Epoch 13 Batch 1500 Loss 5.5373 Accuracy 0.1734\n",
      "Epoch 13 Batch 1600 Loss 5.5603 Accuracy 0.1735\n",
      "Epoch 13 Batch 1700 Loss 5.5751 Accuracy 0.1737\n",
      "Epoch 13 Batch 1800 Loss 5.5831 Accuracy 0.1738\n",
      "Epoch 13 Batch 1900 Loss 5.5881 Accuracy 0.1740\n",
      "Epoch 13 Batch 2000 Loss 5.5913 Accuracy 0.1741\n",
      "Epoch 13 Loss 5.5922 Accuracy 0.1742\n",
      "Time taken for 1 epoch: 5462.286986589432 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 5.9723 Accuracy 0.1742\n",
      "Epoch 14 Batch 100 Loss 5.7284 Accuracy 0.1743\n",
      "Epoch 14 Batch 200 Loss 5.6932 Accuracy 0.1745\n",
      "Epoch 14 Batch 300 Loss 5.5913 Accuracy 0.1747\n",
      "Epoch 14 Batch 400 Loss 5.5402 Accuracy 0.1750\n",
      "Epoch 14 Batch 500 Loss 5.5181 Accuracy 0.1752\n",
      "Epoch 14 Batch 600 Loss 5.4956 Accuracy 0.1754\n",
      "Epoch 14 Batch 700 Loss 5.4672 Accuracy 0.1756\n",
      "Epoch 14 Batch 800 Loss 5.4529 Accuracy 0.1758\n",
      "Epoch 14 Batch 900 Loss 5.4520 Accuracy 0.1760\n",
      "Epoch 14 Batch 1000 Loss 5.4584 Accuracy 0.1761\n",
      "Epoch 14 Batch 1100 Loss 5.4649 Accuracy 0.1763\n",
      "Epoch 14 Batch 1200 Loss 5.4645 Accuracy 0.1765\n",
      "Epoch 14 Batch 1300 Loss 5.4679 Accuracy 0.1767\n",
      "Epoch 14 Batch 1400 Loss 5.4638 Accuracy 0.1769\n",
      "Epoch 14 Batch 1500 Loss 5.5007 Accuracy 0.1770\n",
      "Epoch 14 Batch 1600 Loss 5.5215 Accuracy 0.1771\n",
      "Epoch 14 Batch 1700 Loss 5.5343 Accuracy 0.1772\n",
      "Epoch 14 Batch 1800 Loss 5.5409 Accuracy 0.1774\n",
      "Epoch 14 Batch 1900 Loss 5.5455 Accuracy 0.1775\n",
      "Epoch 14 Batch 2000 Loss 5.5502 Accuracy 0.1777\n",
      "Epoch 14 Loss 5.5517 Accuracy 0.1777\n",
      "Time taken for 1 epoch: 5538.386615991592 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 6.1609 Accuracy 0.1777\n",
      "Epoch 15 Batch 100 Loss 5.7129 Accuracy 0.1779\n",
      "Epoch 15 Batch 200 Loss 5.6638 Accuracy 0.1780\n",
      "Epoch 15 Batch 300 Loss 5.5560 Accuracy 0.1782\n",
      "Epoch 15 Batch 400 Loss 5.4989 Accuracy 0.1784\n",
      "Epoch 15 Batch 500 Loss 5.4729 Accuracy 0.1786\n",
      "Epoch 15 Batch 600 Loss 5.4455 Accuracy 0.1788\n",
      "Epoch 15 Batch 700 Loss 5.4182 Accuracy 0.1790\n",
      "Epoch 15 Batch 800 Loss 5.4072 Accuracy 0.1792\n",
      "Epoch 15 Batch 900 Loss 5.4093 Accuracy 0.1794\n",
      "Epoch 15 Batch 1000 Loss 5.4176 Accuracy 0.1796\n",
      "Epoch 15 Batch 1100 Loss 5.4256 Accuracy 0.1797\n",
      "Epoch 15 Batch 1200 Loss 5.4242 Accuracy 0.1799\n",
      "Epoch 15 Batch 1300 Loss 5.4274 Accuracy 0.1801\n",
      "Epoch 15 Batch 1400 Loss 5.4224 Accuracy 0.1803\n",
      "Epoch 15 Batch 1500 Loss 5.4579 Accuracy 0.1803\n",
      "Epoch 15 Batch 1600 Loss 5.4780 Accuracy 0.1805\n",
      "Epoch 15 Batch 1700 Loss 5.4917 Accuracy 0.1806\n",
      "Epoch 15 Batch 1800 Loss 5.4997 Accuracy 0.1807\n",
      "Epoch 15 Batch 1900 Loss 5.5072 Accuracy 0.1809\n",
      "Epoch 15 Batch 2000 Loss 5.5131 Accuracy 0.1810\n",
      "Saving checkpoint for epoch 15 at checkpoints\\ckpt-3\n",
      "Epoch 15 Loss 5.5153 Accuracy 0.1811\n",
      "Time taken for 1 epoch: 5501.756602525711 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        train_step(inp, tar)\n",
    "    \n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "   \n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8497d6b",
   "metadata": {},
   "source": [
    "### Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "daa09bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_article):\n",
    "    input_article = article_tokenizer.texts_to_sequences([input_article])\n",
    "    input_article = tf.keras.preprocessing.sequence.pad_sequences(input_article, maxlen=ENCODER_LEN, \n",
    "                                                                   padding='post', truncating='post')\n",
    "\n",
    "    encoder_input = tf.expand_dims(input_article[0], 0)\n",
    "\n",
    "    decoder_input = [summary_tokenizer.word_index['<sos>']]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(DECODER_LEN):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input, \n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask\n",
    "        )\n",
    "\n",
    "        predictions = predictions[: ,-1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if predicted_id == summary_tokenizer.word_index['<eos>']:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "976dcda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(input_article):\n",
    "    summarized = evaluate(input_article=input_article)[0].numpy()\n",
    "    summarized = np.expand_dims(summarized[1:], 0)  \n",
    "    return summary_tokenizer.sequences_to_texts(summarized)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9596b2f",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0d89a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_article = dataset['test']['article']\n",
    "test_summary = dataset['test']['highlights']\n",
    "\n",
    "test_article = [preprocess_text(x) for x in test_article]\n",
    "test_summary = [preprocess_text(x) for x in test_summary]\n",
    "\n",
    "test_article = ['<SOS> ' + x + ' <EOS>' for x in test_article]\n",
    "test_summary = ['<SOS> ' + x + ' <EOS>' for x in test_summary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b027f40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Headline : \n",
      "  membership gives the icc jurisdiction over alleged crimes committed in palestinian territories since last june israel and the united states opposed the move which could open the door to war crimes investigations against israelis   \n",
      " \n",
      " Predicted Summary : \n",
      " the judge ordered the release of the release of the release of the judges decision was part of the case \n",
      "\n",
      "Real Headline : \n",
      "  theia a bully breed mix was apparently hit by a car whacked with a hammer and buried in a field shes a true miracle dog and she deserves a good life says sara mellado who is looking for a home for theia   \n",
      " \n",
      " Predicted Summary : \n",
      " the dog was found in a garden of the national park in the us when she was shot by a \n",
      "\n",
      "Real Headline : \n",
      "  mohammad javad zarif has spent more time with john kerry than any other foreign minister he once participated in a takeover of the iranian consulate in san francisco the iranian foreign minister tweets in english   \n",
      " \n",
      " Predicted Summary : \n",
      " hassan rouhani said he believes iran is a good thing as he says he is in the middle east and \n",
      "\n",
      "Real Headline : \n",
      "   americans were exposed to the ebola virus while in sierra leone in march another person was diagnosed with the disease and taken to hospital in maryland national institutes of health says the patient is in fair condition after weeks of treatment   \n",
      " \n",
      " Predicted Summary : \n",
      " the two of the patients were taken to hospital in the first time the two of the patients were taken \n",
      "\n",
      "Real Headline : \n",
      "  student is no longer on duke university campus and will face disciplinary review school officials identified student during investigation and the person admitted to hanging the noose duke says the noose made of rope was discovered on campus about am  \n",
      " \n",
      " Predicted Summary : \n",
      " university of michigan university department of information request was investigated by university university university of michigan university student was investigated \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Real Headline : \\n\", test_summary[i][5:-5],\"\\n \\n Predicted Summary : \\n\", summarize(test_article[i]), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
